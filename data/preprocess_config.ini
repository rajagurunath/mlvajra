[LOGS]
path=/opt/apps/DTNX/logs/

[FillMissing]
cont_columns=BerPreFecMax,PhaseCorrectionAve,PmdMin,Qmin,SoPmdAve
inpCol=BerPreFecMax,PhaseCorrectionAve,PmdMin,Qmin,SoPmdAve
outCol=BerPreFecMax_imputed,PhaseCorrectionAve_imputed,PmdMin_imputed,Qmin_imputed,SoPmdAve_imputed
append_for_outcol=imputed
strategy=mean
path=fillmissing_ch6y_ochctp_30
statspath=/opt/apps/DTNX/models/

[FillMissingCategoryWise]
inpCol=BerPreFecMax,PhaseCorrectionAve,PmdMin,Qmin,SoPmdAve
outCol=BerPreFecMax_imputed,PhaseCorrectionAve_imputed,PmdMin_imputed,Qmin_imputed,SoPmdAve_imputed
append_for_outcol=imputed
strategy=mean
category=module
path=/opt/apps/DTNX/preprocessing_models/fillmissing_per_device_ch6y_ochctp_30


[Categorify]
inpCol=module
outCol=module_category
path=/opt/apps/DTNX/preprocessing_models/category_ch6y_ochctp

[vectorizer]
inpCol=BerPreFecMax_imputed,PhaseCorrectionAve_imputed,PmdMin_imputed,Qmin_imputed,SoPmdAve_imputed
outCol=vectorized_features

[Normalize]
cont_columns=BerPreFecMax,PhaseCorrectionAve,PmdMin,Qmin,SoPmdAve
inpCol=vectorized_features
outCol=features_normal
path=normalizer_ch6y_ochctp_30
statspath=/opt/apps/DTNX/models/

[Scale]
inpCol=vectorized_features
outCol=features_scaled
withStd=True
withMean=False
path=scalar_ch6y_ochctp_30

[ScaleCategoryWise]
inpCol=vectorized_features
outCol=features_scaled
withStd=True
withMean=False
category=module
path=/opt/apps/DTNX/preprocessing_models/scalar_per_device_ch6y_ochctp_30
; path=
; csv_path=actuals1.csv
; hive_sql=

[writeData]
parquet_path=preprocessed_actuals_parquet_test
csv_path=preprocessed_actuals.csv
csv_inpCol=features_scaled
csv_outCol=BerPreFecMax,PhaseCorrectionAve,PmdMin,Qmin,SoPmdAve

[HiveUtils]
columns_to_write=
vector_column=vector_out,
table_name=dummy1
read_sql=select * from coeus.performance_metrics limit 10

[CSVUtils]
read_header=True
inferschema=True
read_path=/opt/apps/DTNX/data/performance_2019-01-17_hr_8.csv
write_path=/opt/apps/DTNX/data/example_performance_2019-01-17_hr_8.csv
csv_inpCol=features_scaled
csv_outCol=BerPreFecMax,PhaseCorrectionAve,PmdMin,Qmin,SoPmdAve
columns_to_write=ts,nodeid,section,module
vector_column=features_scaled

[optimusCsvUtils]
read_header=True
inferschema=True
read_path=/opt/apps/DTNX/prod_to_test/data/full_perf2019-01-20_hr_4.csv
write_path=/opt/apps/DTNX/stgdata/data/ex_test.csv
mode=append
write_header=True

[optimusParquetUtils]
read_path=
write_path=
mode=

[ParquetUtils]
read_parquet_path=actuals_parquet_test
write_parquet_path=preprocessed_actuals_parquet_test

[transformData]
_30_DEVICES=13-L1-9,13-L1-5,13-L1-6,13-L1-10,13-L1-3,13-L1-4,13-L1-8,13-L1-2,13-L1-7,13-L1-1,10-L1-9,10-L1-3,10-L1-1,10-L1-5,10-L1-7,10-L1-4,10-L1-10,10-L1-6,10-L1-8,10-L1-2,11-L1-7,11-L1-3,11-L1-8,11-L1-9,11-L1-1,11-L1-6,11-L1-5,11-L1-2,11-L1-4,11-L1-10
reqcolumns=BerPreFecMax,PhaseCorrectionAve,PmdMin,Qmin,SoPmdAve
index_columns=nodeid,section,ts,module
pivot_column=measure
value_column=val


[csv_to_tempdf]
tempdbname=pivottable
sql=SELECT nodeid, section, module, ts, nodename,max(case when measure='Qmin' then val end )Qmin,max(case when measure='PmdMin' then val end )PmdMin,max(case when measure='BerPreFecMax' then val end )BerPreFecMax,max(case when measure='PhaseCorrectionAve' then val end )PhaseCorrectionAve,max(case when measure='SoPmdAve' then val end )SoPmdAve FROM pivottable where measure in ('BerPreFecMax','PhaseCorrectionAve','PmdMin','Qmin','SoPmdAve') group by nodeid, section, module, ts,nodename